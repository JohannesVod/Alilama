{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c10939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visMatr(matrix):\n",
    "    plt.imshow(matrix, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.title('Matrix Visualization')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c1deb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find model!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import Transformer, ModelParameters\n",
    "from data.tokenizer import RegexTokenizer\n",
    "\n",
    "models_folder = \"models\"\n",
    "tokenizer_folder = \"data\"\n",
    "model_name = \"alilama_29M\"\n",
    "\n",
    "tok_path = os.path.join(\"data\", \"tokenizer.model\")\n",
    "try:\n",
    "    tokenizer = RegexTokenizer()\n",
    "    tokenizer.load(tok_path)\n",
    "except:\n",
    "    raise RuntimeError(f\"tokenizer not found. Make sure {tok_path} exists\")\n",
    "\n",
    "device = \"cpu\"\n",
    "token_count = len(tokenizer)\n",
    "model_path = os.path.join(models_folder, model_name+\".pth\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    loaded_state = torch.load(model_path, map_location=device)\n",
    "    inpt_params = loaded_state[\"params\"]\n",
    "    params = ModelParameters(**inpt_params)\n",
    "    print(\"parameters:\", inpt_params)\n",
    "    params.max_seq_len = 10000 # increase as seen in ALiBi paper\n",
    "    model = Transformer(token_count=token_count, device=device, params=params)\n",
    "    model.load_state_dict(loaded_state['state_dict'])\n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    print(\"Could not find model!\")\n",
    "\n",
    "def generateStream(size, inpt_tokens=None, temperature=1):\n",
    "    for res in model.genLazy(size, inpt_tokens=inpt_tokens, temperature=temperature):\n",
    "        l = [res[0][-1]]\n",
    "        yield tokenizer.decode(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf61b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      " I was so excited to get these. They are very soft and warm, and the fur is very soft. I am very happy with them. EOR I love these! They are so soft and warm. I have a hard time finding boots that fit\n"
     ]
    }
   ],
   "source": [
    "# this is the story the model tries to continue:\n",
    "start_story = \"When i was a little kid, i always wanted to have some nice warm Gloves.\" # Don't use a trailing space!!!\n",
    "# REASONING TEST (sometimes works). We expect something like \"come down.\":\n",
    "# start_story = 'If i throw a ball into the air, it will eventually'\n",
    "modelInput = \" EOR \" + start_story\n",
    "temperature = 5 # how deterministic the model decides the next token (0 = random)\n",
    "model.train()\n",
    "as_tens = tokenizer.encode_ordinary(modelInput)\n",
    "print(len(as_tens))\n",
    "as_tens = torch.tensor(as_tens).unsqueeze(0).to(device)\n",
    "for l in generateStream(50, as_tens, temperature=temperature):\n",
    "    print(l, end=\"\")\n",
    "\n",
    "# show attention scores\n",
    "\"\"\"att_scores = model.getAttScores()\n",
    "first_block = att_scores[0]\n",
    "for head in range(first_block.shape[1]):\n",
    "    att = first_block[0,head,:,:].detach().numpy()\n",
    "    visMatr(att)\"\"\"\n",
    "\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
